{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Phase 1: Data Understanding and Preparation\n",
        "## Silicon (SI) Prediction in Hot Metal\n",
        "\n",
        "### Objective\n",
        "Perform comprehensive data exploration and preparation for predicting Silicon content in hot metal production.\n",
        "\n",
        "### Tasks Covered\n",
        "- âœ… Load and examine dataset structure\n",
        "- âœ… Perform exploratory data analysis (EDA)\n",
        "- âœ… Analyze target variable distribution\n",
        "- âœ… Identify data quality issues\n",
        "- âœ… Apply column mapping\n",
        "- âœ… Feature engineering and selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Configure pandas display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Column mapping as provided in the assignment\n",
        "column_mapping = {\n",
        "    'Timestamp': 'Timestamp',\n",
        "    'Oxygen enrichment rate': 'OxEnRa',\n",
        "    'Blast furnace permeability index': 'BlFuPeIn',\n",
        "    'Enriching oxygen flow': 'EnOxFl',\n",
        "    'Cold blast flow': 'CoBlFl',\n",
        "    'Blast momentum': 'BlMo',\n",
        "    'Blast furnace bosh gas volume': 'BlFuBoGaVo',\n",
        "    'Blast furnace bosh gas index': 'BlFuBoGaIn',\n",
        "    'Theoretical combustion temperature': 'ThCoTe',\n",
        "    'Top gas pressure': 'ToGaPr',\n",
        "    'Enriching oxygen pressure': 'EnOxPr',\n",
        "    'Cold blast pressure': 'CoBlPr',\n",
        "    'Total pressure drop': 'ToPrDr',\n",
        "    'Hot blast pressure': 'HoBlPr',\n",
        "    'Actual blast velocity': 'AcBlVe',\n",
        "    'Cold blast temperature': 'CoBlTe',\n",
        "    'Hot blast temperature': 'HoBlTe',\n",
        "    'Top temperature': 'ToTe',\n",
        "    'Blast humidity': 'BlHu',\n",
        "    'Coal injection set value': 'CoInSeVa',\n",
        "    'Fomer SI': 'FoSI',\n",
        "    'SI': 'SI',\n",
        "    'HoBl': 'HoBl',\n",
        "    'ToGasP': 'ToGasP',\n",
        "    'CoBF': 'CoBF'\n",
        "}\n",
        "\n",
        "print(\"Column mapping defined successfully!\")\n",
        "print(f\"Total columns to map: {len(column_mapping)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_excel('../data/dataset.xlsx')\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "\n",
        "# Display basic information about the dataset\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"DATASET OVERVIEW\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Number of rows: {df.shape[0]:,}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")\n",
        "print(f\"Dataset size: {df.size:,} cells\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Quality Analysis\n",
        "print(\"=\"*50)\n",
        "print(\"DATA QUALITY ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing Values Analysis:\")\n",
        "missing_data = df.isnull().sum()\n",
        "missing_percentage = (missing_data / len(df)) * 100\n",
        "missing_info = pd.DataFrame({\n",
        "    'Column': missing_data.index,\n",
        "    'Missing_Count': missing_data.values,\n",
        "    'Missing_Percentage': missing_percentage.values\n",
        "})\n",
        "missing_info = missing_info[missing_info['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
        "\n",
        "if len(missing_info) > 0:\n",
        "    print(f\"\\nColumns with missing values: {len(missing_info)}\")\n",
        "    print(missing_info)\n",
        "else:\n",
        "    print(\"No missing values found!\")\n",
        "\n",
        "# Check data types\n",
        "print(f\"\\nData Types:\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"\\nDuplicate rows: {duplicates}\")\n",
        "\n",
        "# Basic statistics\n",
        "print(f\"\\nBasic Statistics:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target Variable Analysis (SI - Silicon Content)\n",
        "print(\"=\"*50)\n",
        "print(\"TARGET VARIABLE ANALYSIS (SI)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Check if SI column exists\n",
        "si_column = 'SI'\n",
        "if si_column in df.columns:\n",
        "    si_data = df[si_column].dropna()\n",
        "    \n",
        "    print(f\"SI Statistics:\")\n",
        "    print(f\"Count: {len(si_data):,}\")\n",
        "    print(f\"Mean: {si_data.mean():.4f}\")\n",
        "    print(f\"Std: {si_data.std():.4f}\")\n",
        "    print(f\"Min: {si_data.min():.4f}\")\n",
        "    print(f\"25%: {si_data.quantile(0.25):.4f}\")\n",
        "    print(f\"50%: {si_data.quantile(0.50):.4f}\")\n",
        "    print(f\"75%: {si_data.quantile(0.75):.4f}\")\n",
        "    print(f\"Max: {si_data.max():.4f}\")\n",
        "    print(f\"Range: {si_data.max() - si_data.min():.4f}\")\n",
        "    print(f\"Skewness: {si_data.skew():.4f}\")\n",
        "    print(f\"Kurtosis: {si_data.kurtosis():.4f}\")\n",
        "    \n",
        "    # Check for outliers using IQR method\n",
        "    Q1 = si_data.quantile(0.25)\n",
        "    Q3 = si_data.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = si_data[(si_data < lower_bound) | (si_data > upper_bound)]\n",
        "    print(f\"Outliers (IQR method): {len(outliers)} ({len(outliers)/len(si_data)*100:.2f}%)\")\n",
        "    \n",
        "else:\n",
        "    print(f\"SI column not found in dataset!\")\n",
        "    print(f\"Available columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizations for Target Variable\n",
        "if si_column in df.columns:\n",
        "    si_data = df[si_column].dropna()\n",
        "    \n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    fig.suptitle('Silicon (SI) Content Analysis', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # Histogram\n",
        "    axes[0,0].hist(si_data, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0,0].set_title('Distribution of SI Content')\n",
        "    axes[0,0].set_xlabel('SI Content')\n",
        "    axes[0,0].set_ylabel('Frequency')\n",
        "    axes[0,0].axvline(si_data.mean(), color='red', linestyle='--', label=f'Mean: {si_data.mean():.3f}')\n",
        "    axes[0,0].legend()\n",
        "    \n",
        "    # Box plot\n",
        "    axes[0,1].boxplot(si_data, patch_artist=True, \n",
        "                      boxprops=dict(facecolor='lightgreen', alpha=0.7))\n",
        "    axes[0,1].set_title('SI Content Box Plot')\n",
        "    axes[0,1].set_ylabel('SI Content')\n",
        "    \n",
        "    # Time series plot (if timestamp available)\n",
        "    if 'Timestamp' in df.columns:\n",
        "        df_clean = df.dropna(subset=[si_column, 'Timestamp'])\n",
        "        df_clean['Timestamp'] = pd.to_datetime(df_clean['Timestamp'])\n",
        "        df_sorted = df_clean.sort_values('Timestamp')\n",
        "        axes[1,0].plot(df_sorted['Timestamp'], df_sorted[si_column], alpha=0.7, color='purple')\n",
        "        axes[1,0].set_title('SI Content Over Time')\n",
        "        axes[1,0].set_xlabel('Timestamp')\n",
        "        axes[1,0].set_ylabel('SI Content')\n",
        "        axes[1,0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Q-Q plot for normality check\n",
        "    from scipy import stats\n",
        "    stats.probplot(si_data, dist=\"norm\", plot=axes[1,1])\n",
        "    axes[1,1].set_title('Q-Q Plot (Normality Check)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print distribution insights\n",
        "    print(f\"\\nDistribution Insights:\")\n",
        "    print(f\"- Mean SI content: {si_data.mean():.4f}\")\n",
        "    print(f\"- Standard deviation: {si_data.std():.4f}\")\n",
        "    print(f\"- Coefficient of variation: {(si_data.std()/si_data.mean())*100:.2f}%\")\n",
        "    \n",
        "    if abs(si_data.skew()) < 0.5:\n",
        "        print(f\"- Distribution: Approximately normal (skewness: {si_data.skew():.3f})\")\n",
        "    elif abs(si_data.skew()) < 1:\n",
        "        print(f\"- Distribution: Moderately skewed (skewness: {si_data.skew():.3f})\")\n",
        "    else:\n",
        "        print(f\"- Distribution: Highly skewed (skewness: {si_data.skew():.3f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Analysis and Correlation\n",
        "print(\"=\"*50)\n",
        "print(\"FEATURE ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Identify numeric columns (excluding timestamp)\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "if 'Timestamp' in numeric_columns:\n",
        "    numeric_columns.remove('Timestamp')\n",
        "\n",
        "print(f\"Numeric features identified: {len(numeric_columns)}\")\n",
        "print(f\"Features: {numeric_columns}\")\n",
        "\n",
        "# Calculate correlation with target variable\n",
        "if si_column in df.columns and si_column in numeric_columns:\n",
        "    correlations = df[numeric_columns].corr()[si_column].sort_values(key=abs, ascending=False)\n",
        "    \n",
        "    print(f\"\\nTop 10 Features Most Correlated with SI:\")\n",
        "    print(correlations.head(11).to_string())  # 11 to include SI itself\n",
        "    \n",
        "    # Create correlation heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = df[numeric_columns].corr()\n",
        "    \n",
        "    # Create mask for upper triangle\n",
        "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
        "    \n",
        "    # Generate heatmap\n",
        "    sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Identify highly correlated feature pairs (excluding target)\n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(correlation_matrix.columns)):\n",
        "        for j in range(i+1, len(correlation_matrix.columns)):\n",
        "            corr_val = correlation_matrix.iloc[i, j]\n",
        "            if abs(corr_val) > 0.8:  # High correlation threshold\n",
        "                high_corr_pairs.append((correlation_matrix.columns[i], \n",
        "                                      correlation_matrix.columns[j], \n",
        "                                      corr_val))\n",
        "    \n",
        "    if high_corr_pairs:\n",
        "        print(f\"\\nHighly Correlated Feature Pairs (|r| > 0.8):\")\n",
        "        for feat1, feat2, corr in high_corr_pairs:\n",
        "            print(f\"{feat1} <-> {feat2}: {corr:.3f}\")\n",
        "    else:\n",
        "        print(f\"\\nNo highly correlated feature pairs found (|r| > 0.8)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Preprocessing and Feature Engineering\n",
        "print(\"=\"*50)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Handle timestamp\n",
        "if 'Timestamp' in df_processed.columns:\n",
        "    df_processed['Timestamp'] = pd.to_datetime(df_processed['Timestamp'])\n",
        "    \n",
        "    # Extract time-based features\n",
        "    df_processed['Hour'] = df_processed['Timestamp'].dt.hour\n",
        "    df_processed['DayOfWeek'] = df_processed['Timestamp'].dt.dayofweek\n",
        "    df_processed['Month'] = df_processed['Timestamp'].dt.month\n",
        "    df_processed['Quarter'] = df_processed['Timestamp'].dt.quarter\n",
        "    \n",
        "    # Calculate time differences (for lag features)\n",
        "    df_processed = df_processed.sort_values('Timestamp')\n",
        "    df_processed['TimeDiff_Minutes'] = df_processed['Timestamp'].diff().dt.total_seconds() / 60\n",
        "    \n",
        "    print(\"âœ… Time-based features created\")\n",
        "\n",
        "# Handle missing values\n",
        "print(f\"\\nMissing Value Treatment:\")\n",
        "missing_cols = df_processed.isnull().sum()\n",
        "missing_cols = missing_cols[missing_cols > 0]\n",
        "\n",
        "for col in missing_cols.index:\n",
        "    if df_processed[col].dtype in ['float64', 'int64']:\n",
        "        # For numeric columns, use median imputation\n",
        "        median_val = df_processed[col].median()\n",
        "        df_processed[col].fillna(median_val, inplace=True)\n",
        "        print(f\"âœ… {col}: Filled {missing_cols[col]} missing values with median ({median_val:.3f})\")\n",
        "    else:\n",
        "        # For categorical columns, use mode\n",
        "        mode_val = df_processed[col].mode()[0] if len(df_processed[col].mode()) > 0 else 'Unknown'\n",
        "        df_processed[col].fillna(mode_val, inplace=True)\n",
        "        print(f\"âœ… {col}: Filled {missing_cols[col]} missing values with mode ({mode_val})\")\n",
        "\n",
        "# Create lag features for important variables\n",
        "if si_column in df_processed.columns:\n",
        "    # Create lag features for SI (using previous values)\n",
        "    for lag in [1, 2, 3, 5]:\n",
        "        df_processed[f'SI_lag_{lag}'] = df_processed[si_column].shift(lag)\n",
        "    \n",
        "    # Create rolling statistics\n",
        "    for window in [3, 5, 10]:\n",
        "        df_processed[f'SI_rolling_mean_{window}'] = df_processed[si_column].rolling(window=window).mean()\n",
        "        df_processed[f'SI_rolling_std_{window}'] = df_processed[si_column].rolling(window=window).std()\n",
        "    \n",
        "    print(\"âœ… Lag and rolling features created for SI\")\n",
        "\n",
        "print(f\"\\nProcessed dataset shape: {df_processed.shape}\")\n",
        "print(f\"New features added: {df_processed.shape[1] - df.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed data and create summary\n",
        "print(\"=\"*50)\n",
        "print(\"DATA EXPORT AND SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Save processed dataset\n",
        "df_processed.to_csv('../data/processed_dataset.csv', index=False)\n",
        "print(\"âœ… Processed dataset saved to '../data/processed_dataset.csv'\")\n",
        "\n",
        "# Create data summary report\n",
        "summary_report = {\n",
        "    'Original Dataset': {\n",
        "        'Rows': f\"{df.shape[0]:,}\",\n",
        "        'Columns': df.shape[1],\n",
        "        'Missing Values': df.isnull().sum().sum(),\n",
        "        'Duplicates': df.duplicated().sum(),\n",
        "        'Memory Usage (MB)': f\"{df.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
        "    },\n",
        "    'Processed Dataset': {\n",
        "        'Rows': f\"{df_processed.shape[0]:,}\",\n",
        "        'Columns': df_processed.shape[1],\n",
        "        'Missing Values': df_processed.isnull().sum().sum(),\n",
        "        'New Features Added': df_processed.shape[1] - df.shape[1],\n",
        "        'Memory Usage (MB)': f\"{df_processed.memory_usage(deep=True).sum() / 1024**2:.2f}\"\n",
        "    }\n",
        "}\n",
        "\n",
        "if si_column in df.columns:\n",
        "    si_stats = df[si_column].describe()\n",
        "    summary_report['Target Variable (SI)'] = {\n",
        "        'Count': f\"{si_stats['count']:.0f}\",\n",
        "        'Mean': f\"{si_stats['mean']:.4f}\",\n",
        "        'Std': f\"{si_stats['std']:.4f}\",\n",
        "        'Min': f\"{si_stats['min']:.4f}\",\n",
        "        'Max': f\"{si_stats['max']:.4f}\",\n",
        "        'Range': f\"{si_stats['max'] - si_stats['min']:.4f}\"\n",
        "    }\n",
        "\n",
        "# Print summary\n",
        "for section, metrics in summary_report.items():\n",
        "    print(f\"\\n{section}:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PHASE 1 COMPLETION STATUS\")\n",
        "print(\"=\"*50)\n",
        "print(\"âœ… Task 1.1: Data Exploration and Analysis - COMPLETED\")\n",
        "print(\"âœ… Task 1.2: Feature Engineering and Selection - COMPLETED\") \n",
        "print(\"âœ… Task 1.3: Time Series Analysis Setup - COMPLETED\")\n",
        "print(\"\\nðŸŽ¯ Ready for Phase 2: Model Development and Training\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
